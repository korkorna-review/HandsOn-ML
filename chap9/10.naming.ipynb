{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이름 범위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망처럼 매우 복잡한 모델을 다룰 때는 계산 그래프가 수천개의 노드로 인해 어질러지기 쉽다. <br/>\n",
    "이를 피하려면 **이름범위**를 만들어 관련있는 노드들을 그룹으로 묶는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
       "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
       "         3.78800000e+01, -1.22230000e+02],\n",
       "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
       "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
       "         3.78600000e+01, -1.22220000e+02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "rows, cols = housing.data.shape\n",
    "print(rows, cols)\n",
    "\n",
    "housing.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34476576,  0.98214266,  0.62855945, -0.15375759, -0.9744286 ,\n",
       "        -0.04959654,  1.05254828, -1.32783522],\n",
       "       [ 2.33223796, -0.60701891,  0.32704136, -0.26333577,  0.86143887,\n",
       "        -0.09251223,  1.04318455, -1.32284391]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.34476576,  0.98214266,  0.62855945, -0.15375759,\n",
       "        -0.9744286 , -0.04959654,  1.05254828, -1.32783522],\n",
       "       [ 1.        ,  2.33223796, -0.60701891,  0.32704136, -0.26333577,\n",
       "         0.86143887, -0.09251223,  1.04318455, -1.32284391]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 샘플에 대한 편향에 대한 입력 특성 추가\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((rows, 1)), scaled_housing_data]\n",
    "print(scaled_housing_data_plus_bias.shape)\n",
    "scaled_housing_data_plus_bias[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정값\n",
    "n_epochs = 300\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(rows/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산 그래프 정의\n",
    "X = tf.placeholder(tf.float32, shape=(None, cols+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([cols + 1, 1], -1.0, 1.0), name='theta')  #? \n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2 / rows * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MES', mse)  # summary라 불리는 텐서보드가 인식하는 이진 로그 문자열에 쓰기 위한 노드를 그래프에 추가\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # FileWriter 객체를 만들어 로그 디렉터리에 있는 로그 파일에 서머리를 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_2/sub\n",
      "loss_2/mse\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch* n_batches + batch_index)\n",
    "    indices = np.random.randint(rows, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 10.648258\n",
      "Epoch 100 MSE 0.96413857\n",
      "Epoch 200 MSE 0.51825434\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # 변수 초기화\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "            \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 1 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "            save_path = saver.save(sess, './my_naming_model_{}.ckpt'.format(epoch))\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, './my_naming_model_final.ckpt'.format(epoch))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
